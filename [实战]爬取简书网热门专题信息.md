[toc]
# 思路分析
1. 确定初始url
    > [https://www.jianshu.com/recommendations/collections?order_by=hot](https://www.jianshu.com/recommendations/collections?order_by=hot)
1. 该网页使用了异步加载
1. 确定异步加载url
    > [https://www.jianshu.com/recommendations/collections?page=1&order_by=hot](https://www.jianshu.com/recommendations/collections?page=1&order_by=hot)
1. 确定爬取信息
    * 专题名称
    * 专题介绍
    * 收录文章数
    * 关注人数
1. 使用scarpy爬取,并写入mongodb
 
# 代码分析
items.py
```python
from scrapy.item import Item, Field
 
 
class JianshuItem(Item):
    name = Field()
    content = Field()
    article = Field()
    fans = Field()
```
jianshuspider.py
```python
from scrapy.spiders import CrawlSpider
from scrapy.selector import Selector
from scrapy.http import Request
from jianshu.items import JianshuItem
from scrapy.exceptions import CloseSpider
 
 
class jianshu(CrawlSpider):
    name = 'jianshu'
    start_urls = ['https://www.jianshu.com/recommendations/collections?page=1&order_by=hot']
 
    def parse(self, response):
        item = JianshuItem()
        selector = Selector(response)
        infos = selector.xpath('//div[@class="collection-wrap"]')
        for info in infos:
            name = info.xpath('a[1]/h4/text()').extract()[0]
            content = info.xpath('a[1]/p/text()').extract()
            article = info.xpath('div/a/text()').extract()[0]
            fans = info.xpath('div/text()').extract()[0]
 
            if content:
                content = content[0]
            else:
                content = ''
 
 
            item['name'] = name
            item['content'] = content
            item['article'] = article
            item['fans'] = fans
            yield item
        urls = ['https://www.jianshu.com/recommendations/collections?page={}&order_by=hot'.format(str(i)) for i in range(2, 21)]
        for url in urls:
            yield Request(url, callback=self.parse)
```
settings.py
```python
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'
DOWNLOAD_DELAY = 3
ITEM_PIPELINES = {
   'jianshu.pipelines.JianshuPipeline': 300,
}
```
pipelines.py
```python
import pymongo
 
 
class JianshuPipeline(object):
    def __init__(self):
        client = pymongo.MongoClient('localhost', 27017)
        test = client['test']
        jianshu = test['jianshu']
        self.post = jianshu
 
    def process_item(self, item, spider):
        info = dict(item)
        self.post.insert(info)
        return item
```
main.py
```python
from scrapy import cmdline
 
cmdline.execute('scrapy crawl jianshu'.split())
```
